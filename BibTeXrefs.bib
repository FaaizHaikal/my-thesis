@techReport{RoboCupHumanoidLeague2025,
   author = {RoboCup Humanoid League},
   month = {4},
   title = {RoboCup Soccer Humanoid League Laws of the Game 2025},
   year = {2025}
}
@article{Kok2019,
   abstract = {&lt;p&gt;This paper presents a survey on existing stereo vision algorithms. The existing stereo vision algorithms are discussed in terms of concept, performance and related improvements. Also, a brief analysis of performance comparison among existing stereo vision algorithms is presented. Moreover, available improvements and solutions for stereo vision challenges such as computational complexity, occlusion, radiometric distortion, depth discontinuity and textureless region are reviewed.&lt;/p&gt;},
   author = {Kai Yit Kok and Parvathy Rajendran},
   doi = {10.37936/ecti-cit.2019132.194324},
   issue = {2},
   journal = {ECTI Transactions on Computer and Information Technology (ECTI-CIT)},
   month = {8},
   pages = {112-128},
   title = {A Review on Stereo Vision Algorithm: Challenges and Solutions},
   volume = {13},
   url = {https://ph01.tci-thaijo.org/index.php/ecticit/article/view/194324},
   year = {2019}
}
@article{Wofk2019,
   abstract = {Depth sensing is a critical function for robotic tasks such as localization, mapping and obstacle detection. There has been a significant and growing interest in depth estimation from a single RGB image, due to the relatively low cost and size of monocular cameras. However, state-of-the-art single-view depth estimation algorithms are based on fairly complex deep neural networks that are too slow for real-time inference on an embedded platform, for instance, mounted on a micro aerial vehicle. In this paper, we address the problem of fast depth estimation on embedded systems. We propose an efficient and lightweight encoder-decoder network architecture and apply network pruning to further reduce computational complexity and latency. In particular, we focus on the design of a low-latency decoder. Our methodology demonstrates that it is possible to achieve similar accuracy as prior work on depth estimation, but at inference speeds that are an order of magnitude faster. Our proposed network, FastDepth, runs at 178 fps on an NVIDIA Jetson TX2 GPU and at 27 fps when using only the TX2 CPU, with active power consumption under 10 W. FastDepth achieves close to state-of-the-art accuracy on the NYU Depth v2 dataset. To the best of the authors' knowledge, this paper demonstrates real-time monocular depth estimation using a deep neural network with the lowest latency and highest throughput on an embedded platform that can be carried by a micro aerial vehicle.},
   author = {Diana Wofk and Fangchang Ma and Tien-Ju Yang and Sertac Karaman and Vivienne Sze},
   month = {3},
   title = {FastDepth: Fast Monocular Depth Estimation on Embedded Systems},
   url = {http://arxiv.org/abs/1903.03273},
   year = {2019}
}
@inproceedings{dosSantosCaraba2025,
   author = {André Felipe dos Santos Caraíba and Alisson Assis Cardoso},
   doi = {10.1109/CROS66186.2025.11066153},
   booktitle = {2025 Brazilian Conference on Robotics (CROS)},
   keywords = {Deep learning;Navigation;Depth measurement;Robot vision systems;Estimation;Object detection;Artificial neural networks;Cameras;Feature extraction;Vectors;distance estimation;deep learning;object detection;monocular cameras;deep neural networks;MiDaS;YOLOv8;regression;depth maps},
   pages = {1-6},
   title = {Object Distance Estimation System with Deep Neural Networks},
   volume = {1},
   year = {2025}
}
@article{Zhang2025,
   abstract = {Monocular Depth Estimation (MDE) enables spatial understanding, 3D reconstruction, and autonomous navigation, yet deep learning approaches often predict only relative depth without a consistent metric scale. This limitation reduces reliability in applications such as visual SLAM, precise 3D modeling, and view synthesis. Monocular Metric Depth Estimation (MMDE) overcomes this challenge by producing depth maps with absolute scale, ensuring geometric consistency and enabling deployment without additional calibration. This survey reviews the evolution of MMDE, from geometry-based methods to state-of-the-art deep models, with emphasis on the datasets that drive progress. Key benchmarks, including KITTI, NYU-D, ApolloScape, and TartanAir, are examined in terms of modality, scene type, and application domain. Methodological advances are analyzed, covering domain generalization, boundary preservation, and the integration of synthetic and real data. Techniques such as unsupervised and semi-supervised learning, patch-based inference, architectural innovations, and generative modeling are evaluated for their strengths and limitations. By synthesizing current progress, highlighting the importance of high-quality datasets, and identifying open challenges, this survey provides a structured reference for advancing MMDE and supporting its adoption in real-world computer vision systems.},
   author = {Jiuling Zhang},
   month = {8},
   title = {Survey on Monocular Metric Depth Estimation},
   url = {http://arxiv.org/abs/2501.11841},
   year = {2025}
}
@misc{Macenski2022,
   abstract = {The next chapter of the robotics revolution is well underway with the deployment of robots for a broad range of commercial use cases. Even in a myriad of applications and environments, there exists a common vocabulary of components that robots share—the need for a modular, scalable, and reliable architecture; sensing; planning; mobility; and autonomy. The Robot Operating System (ROS) was an integral part of the last chapter, demonstrably expediting robotics research with freely available components and a modular framework. However, ROS 1 was not designed with many necessary production-grade features and algorithms. ROS 2 and its related projects have been redesigned from the ground up to meet the challenges set forth by modern robotic systems in new and exploratory domains at all scales. In this Review, we highlight the philosophical and architectural changes of ROS 2 powering this new chapter in the robotics revolution. We also show through case studies the influence ROS 2 and its adoption has had on accelerating real robot systems to reliable deployment in an assortment of challenging environments.},
   author = {Steven Macenski and Tully Foote and Brian Gerkey and Chris Lalancette and William Woodall},
   doi = {10.1126/scirobotics.abm6074},
   issn = {24709476},
   issue = {66},
   journal = {Science Robotics},
   month = {5},
   pmid = {35544605},
   publisher = {American Association for the Advancement of Science},
   title = {Robot Operating System 2: Design, architecture, and uses in the wild},
   volume = {7},
   year = {2022}
}
@article{SatyaWidodo2012,
   abstract = {Abstrak Sistem lokalisasi robot sepakbola berbasis mesin visi, bertujuan untuk membuat rancang bangun sistem lokalisasi robot sepakbola. Upaya untuk mewujudkan gagasan ini dilatarbelakangi perlunya sistem lokalisasi pada robot sepakbola agar dapat bermain bola dengan efektif dan efisien. Hasil penelitian yang diharapkan adalah adanya sistem lokalisasi robot sepakbola berbasis kamera dan kompas elektronik. Dengan demikian akan dapat dikembangkan robot sepakbola yang memiliki kemampuan bermain yang lebih baik, karena dengan adanya sistem lokalisasi ini, robot akan dapat menentukan perilaku yang lebih tepat pada kondisi dan lokasi tertentu, misalnya: ketika menemukan bola di daerah sendiri maupun menemukan bola di daerah lawan. Metode penelitian disusun berdasar rancangan sistem yang terdiri dari bagian-bagian utama yaitu: pengambilan gambar/citra lapangan, pemrosesan informasi citra menjadi informasi tentang lokasi robot pada saat itu. Hasil penelitian menunjukkan bahwa sistem lokalisasi pada penelitian ini telah mampu melakukan perkiraan lokasi robot terhadap gawang. Kata kunci: lokalisasi, mesin visi, pengolahan citra, filter HSL Abstract Vision based robot soccer localization system, aiming to create a robot soccer localization system. The implementation of this idea is motivated by the need of the localization system for the robot soccer to play the ball effectively and efficiently. The expected result of this research is to develop the localization system for the robot soccer based on camera vision. Thus the developed robot will be able to play football better. It is because of the localization system that can make the robot able to behave more precisely on certain condition and location. For example, the robot should be able to find the ball in its own area or in the opponent's. Research conducted on several major parts, consisting: image capture, processing image information and directions to the information about the location of the robot at the time. The experimental results indicate that the localization system in this research work has the ability to perform an approximation of the robot location with respect to the goal.},
   author = {Nuryono Satya Widodo and Arif Rahman},
   issn = {1693-6930},
   issue = {4},
   journal = {TELKOMNIKA},
   keywords = {HSL filter,computer vision,image processing,localization},
   pages = {637-644},
   title = {Vision Based Self Localization for Humanoid Robot Soccer},
   volume = {10},
   year = {2012}
}
@article{Xiong2025,
   abstract = {We present Vision in Action (ViA), an active perception system for bimanual robot manipulation. ViA learns task-relevant active perceptual strategies (e.g., searching, tracking, and focusing) directly from human demonstrations. On the hardware side, ViA employs a simple yet effective 6-DoF robotic neck to enable flexible, human-like head movements. To capture human active perception strategies, we design a VR-based teleoperation interface that creates a shared observation space between the robot and the human operator. To mitigate VR motion sickness caused by latency in the robot's physical movements, the interface uses an intermediate 3D scene representation, enabling real-time view rendering on the operator side while asynchronously updating the scene with the robot's latest observations. Together, these design elements enable the learning of robust visuomotor policies for three complex, multi-stage bimanual manipulation tasks involving visual occlusions, significantly outperforming baseline systems.},
   author = {Haoyu Xiong and Xiaomeng Xu and Jimmy Wu and Yifan Hou and Jeannette Bohg and Shuran Song},
   month = {6},
   title = {Vision in Action: Learning Active Perception from Human Demonstrations},
   url = {http://arxiv.org/abs/2506.15666},
   year = {2025}
}
@article{Bertozzi1998,
   author = {M Bertozzi and A Broggi},
   doi = {10.1109/83.650851},
   issue = {1},
   journal = {IEEE Transactions on Image Processing},
   keywords = {Gold;Real time systems;Stereo vision;Vehicle detection;Hardware;Road vehicles;Land vehicles;Software architecture;Vehicle safety;Road safety},
   pages = {62-81},
   title = {GOLD: a parallel real-time stereo vision system for generic obstacle and lane detection},
   volume = {7},
   year = {1998}
}
@article{Chen2024,
   abstract = {Understanding and reasoning about spatial relationships is a fundamental capability for Visual Question Answering (VQA) and robotics. While Vision Language Models (VLM) have demonstrated remarkable performance in certain VQA benchmarks, they still lack capabilities in 3D spatial reasoning, such as recognizing quantitative relationships of physical objects like distances or size differences. We hypothesize that VLMs' limited spatial reasoning capability is due to the lack of 3D spatial knowledge in training data and aim to solve this problem by training VLMs with Internet-scale spatial reasoning data. To this end, we present a system to facilitate this approach. We first develop an automatic 3D spatial VQA data generation framework that scales up to 2 billion VQA examples on 10 million real-world images. We then investigate various factors in the training recipe, including data quality, training pipeline, and VLM architecture. Our work features the first internet-scale 3D spatial reasoning dataset in metric space. By training a VLM on such data, we significantly enhance its ability on both qualitative and quantitative spatial VQA. Finally, we demonstrate that this VLM unlocks novel downstream applications in chain-of-thought spatial reasoning and robotics due to its quantitative estimation capability. Project website: https://spatial-vlm.github.io/},
   author = {Boyuan Chen and Zhuo Xu and Sean Kirmani and Brian Ichter and Danny Driess and Pete Florence and Dorsa Sadigh and Leonidas Guibas and Fei Xia},
   month = {1},
   title = {SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities},
   url = {http://arxiv.org/abs/2401.12168},
   year = {2024}
}
@article{Menegatti2009,
   author = {Emanuele Menegatti and Sven Behnke and Changjiu Zhou},
   doi = {10.1016/J.ROBOT.2009.03.003},
   issn = {0921-8890},
   issue = {8},
   journal = {Robotics and Autonomous Systems},
   month = {7},
   pages = {759-760},
   publisher = {North-Holland},
   title = {Humanoid soccer robots},
   volume = {57},
   url = {https://www.sciencedirect.com/science/article/abs/pii/S0921889009000529},
   year = {2009}
}
@article{Li2024,
   abstract = {Online High-Definition (HD) maps have emerged as the preferred option for autonomous driving, overshadowing the counterpart offline HD maps due to flexible update capability and lower maintenance costs. However, contemporary online HD map models embed parameters of visual sensors into training, resulting in a significant decrease in generalization performance when applied to visual sensors with different parameters. Inspired by the inherent potential of Inverse Perspective Mapping (IPM), where camera parameters are decoupled from the training process, we have designed a universal map generation framework, GenMapping. The framework is established with a triadic synergy architecture, including principal and dual auxiliary branches. When faced with a coarse road image with local distortion translated via IPM, the principal branch learns robust global features under the state space models. The two auxiliary branches are a dense perspective branch and a sparse prior branch. The former exploits the correlation information between static and moving objects, whereas the latter introduces the prior knowledge of OpenStreetMap (OSM). The triple-enhanced merging module is crafted to synergistically integrate the unique spatial features from all three branches. To further improve generalization capabilities, a Cross-View Map Learning (CVML) scheme is leveraged to realize joint learning within the common space. Additionally, a Bidirectional Data Augmentation (BiDA) module is introduced to mitigate reliance on datasets concurrently. A thorough array of experimental results shows that the proposed model surpasses current state-of-the-art methods in both semantic mapping and vectorized mapping, while also maintaining a rapid inference speed. The source code will be publicly available at https://github.com/lynn-yu/GenMapping.},
   author = {Siyu Li and Kailun Yang and Hao Shi and Song Wang and You Yao and Zhiyong Li},
   month = {9},
   title = {GenMapping: Unleashing the Potential of Inverse Perspective Mapping for Robust Online HD Map Construction},
   url = {http://arxiv.org/abs/2409.08688},
   year = {2024}
}
@article{Siciliano2010,
   author = {Bruno Siciliano and Lorenzo Sciavicco and Luigi Villani and Giuseppe Oriolo},
   title = {Robotics: Modelling, Planning and Control},
   year = {2010}
}
@book{Salomon2019,
   author = {David Salomon},
   isbn = {1846283922},
   publisher = {Scholars Portal},
   title = {Transformations and Projections in Computer Graphics},
   year = {2019}
}
@inproceedings{Rudzitis2021,
   abstract = {This paper presents a simple novel IPM matrix computation method, which does not require any additional solutions to calculate homography matrices. This method only requires determining the distance from camera to projecting surface. It's simplicity allows execution even on very small energy-efficient embedded device processors without use of complex libraries such as OpenCV, that cannot be deployed there. Furthermore, it has strictly analytical solution, which allows avoidance of false optimums that are probable in heuristic methods for optimization. We provide formulas, that are easy transferable to any device and can be calculated using standard math libraries.},
   author = {Andrejs Rudzitis and Margarita A. Zaeva},
   doi = {10.1016/j.procs.2021.06.081},
   issn = {18770509},
   booktitle = {Procedia Computer Science},
   keywords = {Bird eye view,Homography,IPM,calibration,projection},
   month = {7},
   pages = {695-700},
   publisher = {Elsevier B.V.},
   title = {Alternative Inverse Perspective Mapping Homography Matrix Computation for ADAS Systems Using Camera Intrinsic and Extrinsic Calibration Parameters},
   volume = {190},
   year = {2021}
}
@techReport{Stewart2021,
   author = {James Stewart and Daniel Clegg and Saleem Watson},
   title = {Calculus Early Transcendentals},
   year = {2021}
}
@techReport{Craig2005,
   author = {John J Craig and Pearson Prentice and Pearson Prentice Hall},
   title = {Introduction to Robotics Mechanics and Control Third Edition},
   year = {2005}
}
@article{Mark2024,
   abstract = {Inverse perspective mapping (IPM) is a crucial technique in camera-based autonomous driving, transforming the perspective view captured by the camera into a bird’s-eye view. This can be beneficial for accurate environmental perception, path planning, obstacle detection, and navigation. IPM faces challenges such as distortion and inaccuracies due to varying road inclinations and intrinsic camera properties. Herein, we revealed inaccuracies inherent in our current IPM approach so proper correction techniques can be applied later. We aimed to explore correction possibilities to enhance the accuracy of IPM and examine other methods that could be used as a benchmark or even a replacement, such as stereo vision and deep learning-based monocular depth estimation methods. With this work, we aimed to provide an analysis and direction for working with IPM.},
   author = {Norbert Markó and Péter Kőrös and Miklós Unger},
   doi = {10.3390/engproc2024079067},
   issn = {26734591},
   issue = {1},
   journal = {Engineering Proceedings},
   keywords = {deprojection,distance estimation,distance estimation correction,inverse perspective mapping},
   publisher = {Multidisciplinary Digital Publishing Institute (MDPI)},
   title = {Inverse Perspective Mapping Correction for Aiding Camera-Based Autonomous Driving Tasks},
   volume = {79},
   year = {2024}
}
@article{Park2023,
   abstract = {This paper proposes an Extended Adaptive Inverse Perspective Mapping (EA-IPM) model that can obtain an accurate bird's-eye view (BEV) from the forward-looking camera on the sidewalk with various curves.},
   author = {Jooyong Park and Younggun Cho},
   doi = {10.7746/jkros.2023.18.1.059},
   issn = {1975-6291},
   issue = {1},
   journal = {Journal of Korea Robotics Society},
   month = {3},
   pages = {59-65},
   publisher = {The Korea Robotics Society},
   title = {Extended and Adaptive Inverse Perspective Mapping for Ground Representation of Autonomous Mobile Robot},
   volume = {18},
   year = {2023}
}
@techReport{Dias2023,
   abstract = {[0009−0008−2410−0045] , Marcos Ricardo Omena de Albuquerque Máximo 1[0000−0001−5375−1076] , Takashi Yoneyama 2[0000−0003−2944−4476] , Davi Abstract. This paper proposes a method to calibrate the model used for inverse perspective mapping of humanoid robots. It aims at providing a reliable way to determine the robot's position given the known objects around it. The position of the objects can be calculated using coordinate transforms applied to the data from the robot's vision device. Those transforms are dependent on the robot's joint angles (such as knee, hip) and the length of some components (e.g. torso, thighs, calves). In practice, because of the sensitivity of the transforms with respect to the inaccuracies of the mechanical data, this calculation may yield errors that make it inadequate for the purpose of determining the objects' positions. The proposed method reduces those errors using an optimization algorithm that can find offsets that can compensate those mechanical inaccuracies. Using this method, a kid-sized humanoid robot was able to determine the position of objects up to 2 meters away from the itself with an average of 3.4 cm of error.},
   author = {Francisco Bruno Dias and Ribeiro Da Silva and Herculano Vasconcelos Barroso and Rodrigo Tanaka Aki},
   title = {Calibration of Inverse Perspective Mapping for a Humanoid Robot},
   year = {2023}
}
@article{Kuipers1999,
   author = {Jack B. Kuipers},
   title = {Quaternions and Rotation Sequences: A Primer with Applications to Orbits, Aerospace and Virtual Reality},
   year = {1999}
}
@techReport{Szeliski2021,
   author = {Richard Szeliski},
   title = {Computer Vision: Algorithms and Applications 2nd Edition},
   url = {https://szeliski.org/Book,},
   year = {2021}
}
@techReport{Kajita2014,
   author = {Shuuji Kajita and Hirohisa Hirukawa and Kensuke Harada and Kazuhito Yokoi},
   title = {Introduction to Humanoid Robotics},
   url = {http://www.springer.com/series/5208},
   year = {2014}
}
@book{Hartley2004,
   author = {Richard. Hartley and Andrew. Zisserman},
   isbn = {9780521540513},
   publisher = {Cambridge University Press},
   title = {Multiple View Geometry in Computer Vision},
   year = {2004}
}
